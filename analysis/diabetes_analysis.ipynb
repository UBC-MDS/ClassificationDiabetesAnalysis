{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1ccc1e2-a61b-46f8-93bd-858632081bbd",
   "metadata": {},
   "source": [
    "# Predicting Diabetes in Pima Indian Women Using Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8be4d5-b89d-4d82-8764-eff066fd63ce",
   "metadata": {},
   "source": [
    "By Inder Khera, Javier Martinez, Jenny Zhang & Jessica Kuo (alphabetically ordered), 2024/11/23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56ae3098-c2d3-49f8-8019-b8ed4ddd78df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "import altair as alt\n",
    "import altair_ally as aly\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import (\n",
    "    RandomizedSearchCV,\n",
    "    cross_validate,\n",
    "    cross_val_score,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import pandera as pa\n",
    "from deepchecks.tabular import Dataset\n",
    "from deepchecks.tabular.checks import (\n",
    "    ClassImbalance, \n",
    "    PercentOfNulls,\n",
    "    OutlierSampleDetection,\n",
    "    DataDuplicates,\n",
    "    MixedDataTypes,\n",
    "    FeatureLabelCorrelation, \n",
    "    FeatureFeatureCorrelation\n",
    ")\n",
    "from deepchecks.tabular.checks.data_integrity import PercentOfNulls\n",
    "\n",
    "import warnings\n",
    "import json\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f464a8d-35d0-4b26-8013-3f6481017712",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This study evaluated logistic regression for predicting diabetes in Pima Indian women using features such as glucose levels, BMI, and pregnancies. The model achieved 75% accuracy on the test set, outperforming the baseline dummy classifier's 67.20%. Glucose was the most significant predictor, followed by BMI and pregnancies, while blood pressure and insulin had weaker impacts. The model misclassified 54 cases, with 35 false negatives and 19 false positives, highlighting areas for improvement.\n",
    "\n",
    "The results indicate that logistic regression is a promising tool for diabetes screening, providing an efficient way to identify potential cases. However, the high number of false negatives is concerning, as they could lead to delayed diagnoses and treatments. Future improvements could include feature engineering to address misclassifications, testing alternative machine learning models, and incorporating additional data, such as lifestyle or genetic factors. Adding probability estimates for predictions could also enhance its clinical usability by helping prioritize further diagnostic tests. These steps could make the model more reliable and practical for real-world healthcare applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8377f0b6-b44c-4a63-94aa-d4f7ed1496dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Diabetes is a serious chronic disease characterized by high levels of glucose in the blood, caused by either insufficient insulin production by the pancreas or the bodyâ€™s inability to effectively use insulin. It has become a significant global health issue, with its prevalence nearly doubling since 1980, and in 2022, 14% of adults aged 18 and older were diagnosed with diabetes, doubling from 7% in 1990 (World Health Organization). Diabetes can lead to severe complications, including blindness, kidney failure, heart attacks, strokes, and lower limb amputations. Early detection enables timely interventions, reduces complications, lowers healthcare costs, and improves quality of life and long-term outcomes (Marshall & Flyvbjerg, 2006).\n",
    "\n",
    "Artificial intelligence (AI) leverages computer systems and big data to simulate intelligent behavior with minimal human intervention, and within it, machine learning (ML) is a subset of AI methodologies. Since the rise of AI, Machine learning has increasingly been applied in various areas of disease detection and prevention in the healthcare field (Bini, 2018). Numerous machine learning techniques have been deployed to develop more efficient and effective methods for diagnosing chronic diseases (Battineni, Chinatalapudi, & Amenta, 2020). Utilizing machine learning methods in diabetes research has been proven to be a critical strategy for harnessing large volumes of diabetes-related data to extract valuable insights (Agarwal & Vadiwala, 2022). Therefore, The goal of this report is to leverage a supervised machine learning model, logistic regression (LR), to evaluate its predictive performance in diagnosing diabetes using a real-world dataset focused specifically on Pima Indian women aged 21 and older."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5224d78-2cb5-42aa-868c-9c62b55ea98c",
   "metadata": {},
   "source": [
    "# Methods and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e615259-0219-4e20-b64a-017acd79182e",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The dataset that was used for the analysis of this project was created by Jack W Smith, JE Everhart, WC Dickson, WC Knowler, RS Johannes and sourced from the National Librabry of Medicine database from the National Institues of Health. Access to their respective analysis can be found [here](https://pmc.ncbi.nlm.nih.gov/articles/PMC2245318/) and access to the dataset can be found via [kaggle](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database/data) (Dua & Graff, 2017). The primary objective of the dataset is to enable diagnostic prediction of whether a patient has diabetes based on specific diagnostic measurements. To ensure consistency and relevance, several constraints were applied to the selection of data instances. Specifically, the dataset includes only female patients who are at least 21 years old and of Pima Indian heritage.\n",
    "\n",
    "Each row/obersvation from the dataset is an individual that identifies to be a part of the Pima (also known as The Akimel O'odham) Indeginous group, located mainly in the Central and Southern regions of the United States. Each observation recorded has summary statistics regarding features that include the Age, BMI, Blood Pressure, Number of Pregnancies, as well as The Diabetes Pedigree Function (which is a score that gives an idea about how much correlation is between person with diabetes and their family history). The dataset offers comprehensive feastures for machine learning analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec82c1f-10ea-4e5e-8728-81d6c77b8053",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "Logistic Regression was employed to develop a classification model for predicting whether the patient is diabetic or not (as indicated in the `outcome` column of the dataset). All variables from the original dataset were used to train the model. The data was split into 70% for the training set and 30% for the testing set. Hyperparameter tuning was performed using `RandomizedSearchCV`, with the accuracy score serving as the classification metric. All variables were standardized just before model fitting. The analysis was conducted using the Python programming language (Van Rossum and Drake, 2009) and several Python packages: numpy (Harris et al., 2020), Pandas (McKinney, 2010), altair (VanderPlas, 2018), altair_ally (Ostblom, 2021) and scikit-learn (Pedregosa et al., 2011). The code used for this analysis and report is available at: https://github.com/UBC-MDS/diabetes_predictor_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e4c56b9-31ae-4b44-aba4-d4be1e4349b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/diabetes.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# load data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df_original \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/diabetes.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m df_original\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\diabetes_predictor\\envs\\diabetes_predictor\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\diabetes_predictor\\envs\\diabetes_predictor\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\diabetes_predictor\\envs\\diabetes_predictor\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\diabetes_predictor\\envs\\diabetes_predictor\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\diabetes_predictor\\envs\\diabetes_predictor\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/diabetes.csv'"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "df_original = pd.read_csv('../data/diabetes.csv')\n",
    "df_original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13b0c2c-57b0-4d6c-8ed7-86eb5b0d4257",
   "metadata": {},
   "source": [
    "## Validation Ranges\n",
    "- For `Outcome` each observation must have a value of either 0 or 1\n",
    "- For `Pregnancies` we have chosen the range of 0-15; although having 15 pregnancies is very unlikely there has been rare historic cases for this upper limit. Anything beyond 15 is too high of an outlier to keep within the dataset.\n",
    "- For `Glucose`, we chose the range of 50-240 as any glucose level below 50 and above 240 would require immediate medical attention, therefore these values are outliers that are not sustainable and in turn not fit for our model.\n",
    "- For `BloodPressure` the acceptable range of 40-180 was chosen as any blood pressure level below 40 will cause dizziness, fainting, shock-like symptoms, cold/clammy skin, or rapid breathing. Meanwhile anything above 180 is a dangerourly high level and a Hypertensive Crisis. In both cases immediate medical attention is required therefore these values are not suitable for our classification model.\n",
    "- For `SkinThickness` (in terms of the Triceps' skin fold) the thickness of skin cannot be below 0 mm and any level of thickness above 80 mm is a very extreme and unlikely level of obesity. Meaning any value below 0 mm would be an error and any value above 80 mm is a very high outlier\n",
    "- For `Insulin` the acceptable range is 0-800. Although having a insulin level of 0 is unlikely there are instances where the insulin level is so low that it is not detected during a test giving the value of 0. Regarding the upper limit of 700 anything above this range considered severe hyperglycemia and needs immediate emergency treatment, thus not suitable when looking at ranges that do not include outliers\n",
    "- For `BMI` the range of 0-65 was chosen as your BMI cannot be lower than 0 and going over 65 is an extreme case of obesity.\n",
    "- For `DiabetesPedigreeFunction` repersents the genetic risk of diabetes based on family history. Therefore this value cannot be smaller than 0 (which repersents having no familial diabetic history), meanwhile the upper limit of 2.5 repersents having both parents and other close family members having diabetic history, which is an unlikely value to encounter.\n",
    "- For `Age` we have chosen the range 18-90 as this test was conducted on adults, we have chosen the upper limit of 90 as it is unlikely for someone to be above 90. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b40784",
   "metadata": {},
   "source": [
    "The approach to data validation in this analysis was inspired by concepts presented in Tiffany A. Timbers' [breast-cancer-predictor GitHub repository](https://github.com/ttimbers/breast-cancer-predictor/tree/1.0.0), which uses libraries such as deepchecks and pandera for validating data quality and structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a4e73f-527e-44cc-964c-81df22faba27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename=\"validation_errors.log\",\n",
    "    filemode=\"w\",\n",
    "    format=\"%(asctime)s - %(message)s\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "# Define schema\n",
    "schema = pa.DataFrameSchema(\n",
    "    { \n",
    "        \"Outcome\": pa.Column(int, pa.Check.isin([0, 1])),\n",
    "        \"Pregnancies\": pa.Column(int, pa.Check.between(0, 15), nullable=True),\n",
    "        \"Glucose\": pa.Column(int, pa.Check.between(50, 240), nullable=True),\n",
    "        \"BloodPressure\": pa.Column(int, pa.Check.between(40, 180), nullable=True),\n",
    "        \"SkinThickness\": pa.Column(int, pa.Check.between(0, 80), nullable=True),\n",
    "        \"Insulin\": pa.Column(int, pa.Check.between(0, 800), nullable=True),\n",
    "        \"BMI\": pa.Column(float, pa.Check.between(0, 65), nullable=True),\n",
    "        \"DiabetesPedigreeFunction\": pa.Column(float, pa.Check.between(0, 2.5), nullable=True),\n",
    "        \"Age\": pa.Column(int, pa.Check.between(18, 90), nullable=True),\n",
    "    },\n",
    "    checks=[\n",
    "        pa.Check(lambda df: ~df.duplicated().any(), error=\"Duplicate rows found.\"),\n",
    "        pa.Check(lambda df: ~(df.isna().all(axis=1)).any(), error=\"Empty rows found.\")\n",
    "    ],\n",
    "    drop_invalid_rows=False,  # Ensure this is properly closed\n",
    ")\n",
    "\n",
    "# Initialize error cases DataFrame\n",
    "error_cases = pd.DataFrame()\n",
    "data = df_original.copy()\n",
    "\n",
    "# Validate data and handle errors\n",
    "try:\n",
    "    validated_data = schema.validate(data, lazy=True)\n",
    "except pa.errors.SchemaErrors as e:\n",
    "    error_cases = e.failure_cases\n",
    "\n",
    "    # Convert the error message to a JSON string\n",
    "    error_message = json.dumps(e.message, indent=2)\n",
    "    logging.error(\"\\n\" + error_message)\n",
    "\n",
    "# Filter out invalid rows based on the error cases\n",
    "if not error_cases.empty:\n",
    "    invalid_indices = error_cases[\"index\"].dropna().unique()\n",
    "    df = (\n",
    "        data.drop(index=invalid_indices)\n",
    "        .reset_index(drop=True)\n",
    "        .drop_duplicates()\n",
    "        .dropna(how=\"all\")\n",
    "    )\n",
    "else:\n",
    "    df = data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f07bf1-1ec7-43f8-b70e-8552eadb8474",
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    "\n",
    "df.to_csv('../data/processed/df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c19426-8ff5-4c00-aeb9-21335e7904fe",
   "metadata": {},
   "source": [
    "The `shape` attribute shows us the number of observations and the number of features in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97143fb-aad8-4ee4-9e0c-43e27912a360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA\n",
    "print(df_original.shape)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7210497-6dcb-4383-8c56-cba0f6f2f243",
   "metadata": {},
   "source": [
    "We do see 49 observations being dropped post our preliminary data validation as the rows dropped contain meaningless and / or invalid data entries that would unlikely provide useful information but instead introduce noise or spurious relationships into our model. Details of the observations dropped can be found at https://github.com/UBC-MDS/diabetes_predictor_py under `validation_errors.log`. It is clearly shown from the log that majority of data points that are dropped contained values of 0 where levels cannot possibly be, hence, we suspect that the values recorded at 0 are likely missing values recorded in such way, a critical point we will confirm with data collectors if we have access. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84b7f2f-44ac-4f95-9324-91a326914a16",
   "metadata": {},
   "source": [
    "The `info()` method shows that the dataset does not have any features with missing values, and all features are numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa0c09d-0057-4303-8ab1-d2848f7906f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b986adf-e01d-4e37-97f2-e0f5eb32060b",
   "metadata": {},
   "source": [
    "Using the `train_test_split()` function we will split our data set with 70% going to train the model and 30% going towards testing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0894088b-5714-4a20-bdb0-f4ffdf786dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the split\n",
    "train_df, test_df = train_test_split(df,\n",
    "                                     train_size = 0.7, \n",
    "                                     random_state=123)\n",
    "\n",
    "df.to_csv('../data/processed/train_df.csv')\n",
    "df.to_csv('../data/processed/test_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bcc2ba-a8de-4a68-9f29-308309a5025f",
   "metadata": {},
   "source": [
    "The `describe()` shows us the summary statistics of each of our features as well as our target value. We can see the mean as well as the spread (standard deviation). Using this information and the visualization tools we will see next we can determine how skewed each of our features are for their respective values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ceec33-f14b-4070-a225-186f75860095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore training data\n",
    "census_summary = train_df.describe()\n",
    "census_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35771738-ed67-4c0a-ade6-de3ff59304a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List features\n",
    "features = census_summary.columns.tolist()\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dec8686-3519-49a2-8b6d-dfb254fb648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature distributions\n",
    "feature_histograms = alt.Chart(train_df).transform_calculate(\n",
    ").mark_bar(opacity=0.5).encode( x = alt.X(alt.repeat()).type(\n",
    "'quantitative').bin(maxbins=30), y= alt.Y('count()').stack(False),\n",
    "                               color = 'Outcome:N'\n",
    ").properties( height=250,\n",
    "width=250 ).repeat(\n",
    "features, columns=2\n",
    ")\n",
    "\n",
    "feature_histograms "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6995241a-3405-46c1-a32c-51096604f88e",
   "metadata": {},
   "source": [
    "<i>Figure 1. Comparison of the empirical distributions of training data predictors between those non-diabetic and diabetic<i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe80c9bb-631f-4ba8-b5ae-1256e15a3d92",
   "metadata": {},
   "source": [
    "Figure 1 above illustrates the distribution of each feature, categorized based on the Outcome variable: 0 (Non-Diabetic) and 1 (Diabetic). This visualization provides insights into the relationships between individual features and the target variable.\n",
    "\n",
    "For the `Glucose` levels, Non-Diabetic class exhibits a roughly normal distribution, whereas the Diabetic class shows a pronounced shift toward the middle-to-higher range of glucose levels. \n",
    "\n",
    "The `BMI` distribution resembles a normal distribution but skews slightly toward higher values. Similar to Glucose levels, the Diabetic class displays a decent shit towards the middle-to-higher ranges when compared to Non-Diabetic class, suggesting the potential for distinct differences between target groups within this category.\n",
    "\n",
    "The `Age` distribution reveals that individuals aged 20 to 32 are predominantly Non-Diabetic. Beyond age 32, the counts of Diabetic and Non-Diabetic individuals become comparable, with some bins showing a higher count for the Diabetic class, despite fewer overall observations in this group. The Non-Diabetic class leans toward younger ages, while the Diabetic class has a more even distribution across its age range.\n",
    "\n",
    "For `Pregnancies`, `Insulin`, and `DiabetesPedigreeFunction` -genetic risk of diabetes based on family history ranging from 0 to 2.5, the lower range of pregnancies is dominated by the Non-Diabetic class, whereas whereas higher numbers are more common in the Diabetic class.\n",
    "\n",
    "For `Blood Pressure` and `Skin Thickness`, both the Diabetic and Non-Diabetic classes approximates a normal distribution; however, the Non-Diabetic distribution skews slightly towards lower values, while the Diabetic class skews more towards higher values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f62fb0-c9e4-4847-9476-49b8cc16c6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate training data for class imbalance for target variable \n",
    "# Do these on training data as part of EDA! \n",
    "train_df_ds = Dataset(train_df, label = 'Outcome', cat_features=[])\n",
    "\n",
    "check_lab_cls_imb = ClassImbalance().add_condition_class_ratio_less_than(0.4)\n",
    "check_lab_cls_imb_result = check_lab_cls_imb.run(dataset = train_df_ds)\n",
    "\n",
    "if check_lab_cls_imb_result.passed_conditions():\n",
    "    raise ValueError(\"Class imbalance exceeds the maximum acceptable threshold.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d395ab0-be41-49dc-b53b-8f9ebfcd4cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate training data for percent of nulls\n",
    "check_pct_nulls = PercentOfNulls().add_condition_percent_of_nulls_not_greater_than(0.05)\n",
    "check_pct_nulls_result = check_pct_nulls.run(dataset = train_df_ds)\n",
    "\n",
    "if not check_pct_nulls_result.passed_conditions():\n",
    "    raise ValueError(\"Percent of nulls exceeds the maximum acceptable threshold for at least one column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6287276b-c645-4044-9d9a-d4849bab9f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate training data for percent of outlier samples using loOP algo\n",
    "check_out_sample = (\n",
    "    OutlierSampleDetection(nearest_neighbors_percent = 0.01, extent_parameter = 3)\n",
    "    .add_condition_outlier_ratio_less_or_equal(max_outliers_ratio = 0.001, outlier_score_threshold = 0.9)\n",
    ")\n",
    "check_out_sample_result = check_out_sample.run(dataset = train_df_ds)\n",
    "\n",
    "if not check_out_sample_result.passed_conditions():\n",
    "    raise ValueError(\"Number of outlier samples exceeds the maximum acceptable threshold.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb0abd7-a03a-4918-b9e9-a98963f99567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate training data for data duplicates\n",
    "# set duplicate condition to 0 as would not expect any two patient with the exact same situation\n",
    "check_data_dup = DataDuplicates().add_condition_ratio_less_or_equal(0)\n",
    "check_data_dup_result = check_data_dup.run(dataset = train_df_ds)\n",
    "\n",
    "if not check_data_dup_result.passed_conditions():\n",
    "    raise ValueError(\"Data duplicates exceed the maximum acceptable threshold.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d226cde9-402e-413d-9515-d2ff0b42e38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate training data for mixed data types across all columns\n",
    "check_mix_dtype = MixedDataTypes().add_condition_rare_type_ratio_not_in_range((0.01, 0.2))\n",
    "check_mix_dtype_result = check_mix_dtype.run(dataset = train_df_ds)\n",
    "\n",
    "if not check_mix_dtype_result.passed_conditions():\n",
    "    # raise a warning instead of an error in this case\n",
    "    warnings.warn(\"Percentage of rare data type in dangerous zone for at least one column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9f6883-e835-4c89-9886-74402cecee80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlations across features\n",
    "aly.corr(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb988de9-549c-4594-ace9-9fa6436bbd27",
   "metadata": {},
   "source": [
    "<i>Figure 2. Pearson and Spearman correlations across all features<i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43927d3-04dc-433f-8c90-d7c662826730",
   "metadata": {},
   "source": [
    "Figure 2 shows the correlation between all of the respective features. The main reasoning to analyze this is to see if there is any multicollinearity between any of the features which cou;d be problamatic when conducting a Logistic Regression. We see that highest level of correlation is between Age and Pregnancies (0.62 via Spearman, and 0.56 by Pearson). Since this is below the threshold of 0.7, we can conclude that all features' coefficients are suitable and will not cause any multicollinearity in our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd50b563-9985-4d2c-8a3f-e9a97f2a6fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize relationships\n",
    "aly.pair(train_df[features].sample(300), color='Outcome:N')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c2d626-c338-4ff2-8de3-8dee97a2e1a7",
   "metadata": {},
   "source": [
    "<i>Figure 3. Pairwise scatterplots between each of features in dataset to visualize relationship<i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567fa9f0-a7b0-4408-8b17-39259c51688c",
   "metadata": {},
   "source": [
    "Figure 3 illustrates the relationships between the features. For the most part, the features do not display noticeable trends. However, Skin Thickness and BMI show a moderate visual relationship, which is intuitive since higher body mass is generally associated with increased skin thickness. \n",
    "\n",
    "Referring back to the correlation graph, Skin Thickness and BMI have a Spearman correlation of 0.44. This value is below the multicollinearity threshold of 0.7, indicating that these features do not pose a risk of multicollinearity in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdff989-3503-4bf1-9684-af7d1b2d1c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate training data for anomalous correlations between target/response variable \n",
    "# and features/explanatory variables, \n",
    "# as well as anomalous correlations between features/explanatory variables\n",
    "\n",
    "check_feat_lab_corr = FeatureLabelCorrelation().add_condition_feature_pps_less_than(0.7)\n",
    "check_feat_lab_corr_result = check_feat_lab_corr.run(dataset = train_df_ds)\n",
    "\n",
    "check_feat_feat_corr = FeatureFeatureCorrelation().add_condition_max_number_of_pairs_above_threshold(threshold = 0.7, n_pairs = 0)\n",
    "check_feat_feat_corr_result = check_feat_feat_corr.run(dataset = train_df_ds)\n",
    "\n",
    "if not check_feat_lab_corr_result.passed_conditions():\n",
    "    raise ValueError(\"Feature-Label correlation exceeds the maximum acceptable threshold.\")\n",
    "\n",
    "if not check_feat_feat_corr_result.passed_conditions():\n",
    "    raise ValueError(\"Feature-feature correlation exceeds the maximum acceptable threshold.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e9d502-d7de-4947-9aef-7237844abf57",
   "metadata": {},
   "source": [
    "Here we further split our dataset into X and y for both the training and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3c1217-fed7-4e33-a18f-4d510e6af191",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(columns = ['Outcome'])\n",
    "y_train = train_df['Outcome']\n",
    "X_test = test_df.drop(columns = ['Outcome'])\n",
    "y_test = test_df['Outcome']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14944f93-65c4-439c-b61d-a515fc5e2e31",
   "metadata": {},
   "source": [
    "The Dummy Classifier acts as our baseline for conductin our initial analysis.\n",
    "The Dummy Baseline gives us a score of around 0.6720."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11f6174-f5d6-4e45-b857-2bfb65a80e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dummy Classifier and cross validation\n",
    "dummy_clf = DummyClassifier()\n",
    "mean_cv_score = cross_val_score(dummy_clf, \n",
    "                                X_train,\n",
    "                                y_train).mean()\n",
    "mean_cv_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4249d6d-92b8-4ab5-9843-b0bfeebf5475",
   "metadata": {},
   "source": [
    "We will use a Logistic Regression model for classification. Given the presence of outliers in our features, it is advisable to apply StandardScaler() to normalize the feature values before fitting the model. This ensures that all features are on a similar scale, improving the model's performance and stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f54d5b3-b4b3-4a69-9080-976088f33aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Logistic Regression pipeline\n",
    "log_pipe=make_pipeline(\n",
    "    StandardScaler(), \n",
    "    LogisticRegression(max_iter=2000,random_state=123)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0644fd-9d0d-4e6b-b732-11d735a769d8",
   "metadata": {},
   "source": [
    "We optimize the hyperparameter `C` for our Logistic Regression model using a random search approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10857b9-e68b-4427-90d0-65638136b431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter optimization\n",
    "np.random.seed(123)\n",
    "param_dist = {\n",
    "    \"logisticregression__C\": loguniform(1e-5, 1e+5) \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51de2544-ffac-4b1b-970a-e1920e03b508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Random Search\n",
    "random_search = RandomizedSearchCV(log_pipe,param_dist,\n",
    "                                   n_iter=20,\n",
    "                                   n_jobs=-1,\n",
    "                                   return_train_score=True,\n",
    "                                   random_state=123)\n",
    "\n",
    "random_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc4ce0d-dbf8-4473-8a0d-a5b9229e363d",
   "metadata": {},
   "source": [
    "Identify the optimal value for the hyperparameter C to be used in our Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64337e46-03eb-4182-bde3-4b7efb3f33e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify optiomized parameter C\n",
    "best_params = random_search.best_params_ \n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b463a207-970a-4b87-b183-165a270216cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display scores\n",
    "pd.DataFrame(random_search.cv_results_).sort_values(\n",
    "    \"rank_test_score\").head(3)[[\"mean_test_score\",\n",
    "                                \"mean_train_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897f9bdd-d070-4595-8bba-525bcce98875",
   "metadata": {},
   "source": [
    "Having determined the best Logistic Regression model for our analysis, we further explore feature importance with coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8c81cf-262e-4ac9-95eb-7b89a0e15385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model from the search\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Retrieve the coefficients and feature names\n",
    "coefficients = best_model.named_steps['logisticregression'].coef_.flatten()\n",
    "features = X_train.columns  \n",
    "\n",
    "# Create a DataFrame to display the feature names and corresponding coefficients\n",
    "coeff_df = pd.DataFrame({\n",
    "    'Features': features,\n",
    "    'Coefficients': coefficients\n",
    "})\n",
    "\n",
    "# Sort by 'Coefficients' in descending order to see the most important features first\n",
    "coeff_df_sorted = coeff_df.sort_values(by = 'Coefficients', ascending = False)\n",
    "\n",
    "# Create a heatmap for the coefficients (we will visualize them as a single column)\n",
    "coeff_df_sorted.style.format(\n",
    "    precision = 3\n",
    ").background_gradient(\n",
    "    axis = None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da25f622-c18d-4042-88c5-c68544ce2fae",
   "metadata": {},
   "source": [
    "Table 1: Logistic regression feature importance measured by coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70be6981-1268-4215-81f1-acfee385e7d6",
   "metadata": {},
   "source": [
    "Based on the heatmap and table 1 above, the feature importance coefficients for the logistic regression model predicting diabetes reveal that `Glucose` (0.72) is the strongest positive influence, followed by `BMI` (0.39), `Pregnancies` (0.23), `Age` (0.19), and `DiabetesPedigreeFunction` (0.16). The negative influence `SkinThickness` (-0.007) along with the remaining positive features `BloodPressure` (0.048) and `Insulin` (0.002), have weak impacts on the prediction, with their effects being less pronounced. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60168556-9d5e-494a-a77f-b83ca2c2f991",
   "metadata": {},
   "source": [
    "We then evaluate the best Logistic Regression model, obtained from the hyperparameter search, on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569ec32e-1d4f-413a-84ba-38359e62a0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the best model\n",
    "y_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97066ce3-28e9-4462-984b-6d045389fa50",
   "metadata": {},
   "source": [
    "In addition, to enhance the model's practical use in a clinical setting, we are providing and reporting probability estimates for the predictions of diabetes. Offering probability estimates would allow clinicians to gauge the model's confidence in its predictions. This would give clinicians the opportunity to conduct additional diagnostic tests if the predicted probability for the outcome (i.e. diagnosis of prediction) is not sufficiently high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f712d3-c220-4f1d-9f4e-75eced342c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = best_model.predict_proba(X_test)\n",
    "pred_bool = (y_test == y_pred)\n",
    "pred_results_1 = np.vstack([y_test, y_pred, pred_bool, y_pred_prob[:, 1]])\n",
    "pred_results_1_df = pd.DataFrame(pred_results_1.T, \n",
    "                                 columns = ['y_test', 'y_pred', 'pred_bool', 'y_pred_prob_1'])\n",
    "pred_results_1_df['pred_bool'] = pred_results_1_df['pred_bool'] == 1\n",
    "pred_results_1_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae4aa67-7176-4eed-9170-2962f09aa935",
   "metadata": {},
   "source": [
    "Our prediction model performed decent on test data, with a final overall accuracy of 0.75. In addition, looking through the prediction results dataframe, there are a total of 54 mistakes. Of which, 35 mistakes were predicting diabetic as non-diabetic (false negatives) and 19 mistakes were made predicting diabetic as non-diabetic (false positives). Considering implementation in clinic, there is room for improvement in the algorithm as false negatives are more harmful than false positives, and we should aim to lower false positives even further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30250d8d-40d4-41e3-8560-48d2b8a56673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy\n",
    "accuracy = best_model.score(X_test, y_test)\n",
    "\n",
    "pd.DataFrame({'accuracy': [accuracy]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e43428-628a-4495-b078-60568d4276e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of correct predictions and misclassifications\n",
    "value_counts = pred_results_1_df['pred_bool'].value_counts()\n",
    "\n",
    "pd.DataFrame({\n",
    "    'correct predictions': [value_counts.get(True, 0)], \n",
    "    'misclassifications': [value_counts.get(False, 0)]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056f09d2-a81b-42a4-bcaa-3837aeee29b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of false positives (FPs) and false negatives (FNs)\n",
    "fp = len(pred_results_1_df[(pred_results_1_df['y_test'] == 0) & (pred_results_1_df['y_pred'] == 1)])\n",
    "fn = len(pred_results_1_df[(pred_results_1_df['y_test'] == 1) & (pred_results_1_df['y_pred'] == 0)])\n",
    "\n",
    "pd.DataFrame({\n",
    "    'false positives': [fp], \n",
    "    'false negatives': [fn]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f77a691-b4fc-41fb-850a-670064fcbc83",
   "metadata": {},
   "source": [
    "Moreover, visualizing prediction probabilities alongside the prediction accuracy for each test sample provides a clearer understanding of the model's performance. This approach allows us to easily assess how well the model predicts, while also highlighting patients who were misdiagnosed. Particularly, it helps us focus on false negatives, as the consequences of these errors are more critical in a clinical context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35218b3b-5536-40f5-909b-5e9f1374d49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(pred_results_1_df, title = 'Test Set Prediction Accuracy').mark_tick().encode(\n",
    "    x = alt.X('y_pred_prob_1').title('Positive Class Prediction Prob'),\n",
    "    y = alt.Y('pred_bool').title('Pred. Accuracy'),\n",
    "    color = alt.Color('y_test:N').title('Outcome')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355be816-f4fe-4416-920c-7f8d922daac3",
   "metadata": {},
   "source": [
    "<i>Figure 4. Test Set Prediction Accuracy by Prediction Probability<i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5157e9-de23-4f0f-bdcd-7e3e508f3572",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "While the performance of this model may be valuable as a screening tool in a clinical context, especially given its improvements over the baseline, there are several opportunities for further enhancement. One potential approach is to closely examine the 54 misclassified observations, comparing them with correctly classified examples from both classes. The objective would be to identify which features may be contributing to the misclassifications and investigate whether feature engineering could help the model improve its predictions on the observations it is currently struggling with. Additionally, we would try seeing whether we can get improved predictions using other classifiers. Other classifiers we might try are 1) random forest because it automatically allows for feature interaction, 2) k-nearest neighbours (k-NN) which usually provides easily interpretable and decent predictions, and 3) support vector classifier (SVC) as it allows for non-linear prediction using the rbf kernel. Finally, there runs the possibility that the features offered from this dataset alone are not sufficient to predict with high accuracy. In this case, conducting additional conversations with data collectors for additional useable information or explore additional datasets that can be joined so our set of features can be expanded for more complicated analysis might be beneficial. \n",
    "\n",
    "At last, we recognize the limitation with this dataset, as it focuses solely on Pima Indian women aged 21 and older, which limits its generalizability to other populations. To improve the analysis, it would be valuable to combine this data with other datasets representing different age groups, genders, and ethnicities, enabling more comprehensive insights and broader applicability of the findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700e47a0-e251-4d4b-830e-f23525e120f9",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In conclusion, this study demonstrated the effectiveness of logistic regression in predicting diabetes among Pima Indian women using diagnostic features such as glucose, BMI, and pregnancies. With an accuracy of 75% on the test set, the model outperformed the baseline Dummy Classifier's 67.20%. Glucose was identified as the most influential predictor, followed by BMI and pregnancies, while features like blood pressure, insulin, and skin thickness had weaker impacts. However, the model's 54 misclassifications, particularly the 35 false negatives, underscore the need for further refinement to minimize the risk of undiagnosed cases.\n",
    "\n",
    "These findings highlight logistic regression's potential as an initial screening tool in clinical settings, offering a data-driven approach to early diabetes detection. Nevertheless, improvements are essential to enhance its accuracy and practical utility. Strategies such as feature engineering, alternative machine learning models, and the incorporation of additional data, such as lifestyle or genetic factors, could further optimize performance. Additionally, providing probability estimates for predictions could enhance clinical decision-making by identifying cases requiring further diagnostics. With these refinements, the model could become a valuable tool for reducing complications and improving outcomes in diabetes care."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973fcb4d-ee7e-4367-a0d6-8e674bf4eb85",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a22e58-2428-4858-bef1-c90c600813d1",
   "metadata": {},
   "source": [
    "Agarwal, N., & Vadiwala, R. (2022). Machine Learning and Data Mining Methods in Diabetes Research. Asian Journal of Organic & Medicinal Chemistry.\n",
    " \n",
    "Battineni, G., Sagaro, G. G., Chinatalapudi, N., & Amenta, F. (2020). Applications of machine learning predictive models in the chronic disease diagnosis. Journal of personalized medicine, 10(2), 21.\n",
    " \n",
    "Bini, S. A. (2018). Artificial intelligence, machine learning, deep learning, and cognitive computing: what do these terms mean and how will they impact health care?. The Journal of arthroplasty, 33(8), 2358-2361.\n",
    " \n",
    "Dua, D., & Graff, C. (2017). Pima Indians Diabetes Database. UCI Machine Learning Repository. Retrieved from https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database/data\n",
    "\n",
    "Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., ... & Oliphant, T. E. (2020). Array programming with NumPy. Nature, 585(7825), 357â€“362. https://doi.org/10.1038/s41586-020-2649-2\n",
    "\n",
    "McKinney, W. (2010). Data structures for statistical computing in Python. In S. van der Walt & J. Millman (Eds.), Proceedings of the 9th Python in Science Conference (pp. 51â€“56).\n",
    "\n",
    "Marshall, S. M., & Flyvbjerg, A. (2006). Prevention and early detection of vascular complications of diabetes. Bmj, 333(7566), 475-480.\n",
    "\n",
    "Ostblom, J. (2021). altair_ally: Enhancing Altair for statistical visualization. Retrieved from https://github.com/jostblom/altair_ally\n",
    "\n",
    "Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., ... & Duchesnay, Ã‰. (2011). Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12(October), 2825â€“2830.\n",
    "\n",
    "VanderPlas, J., Granger, B., Heer, J., Moritz, D., Wongsuphasawat, K., Satyanarayan, A., ... & Sievert, S. (2018). Altair: Interactive statistical visualizations for Python. Journal of Open Source Software, 3(32), 1057. https://doi.org/10.21105/joss.01057\n",
    "\n",
    "Van Rossum, G., & Drake, F. L. (2009). Python 3 reference manual. Scotts Valley, CA: CreateSpace.\n",
    "  \n",
    "World Health Organization. (n.d.). Diabetes. Retrieved November 22, 2024, from https://www.who.int/news-room/fact-sheets/detail/diabetes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:diabetes_predictor-diabetes_predictor]",
   "language": "python",
   "name": "conda-env-diabetes_predictor-diabetes_predictor-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
