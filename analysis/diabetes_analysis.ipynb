{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1ccc1e2-a61b-46f8-93bd-858632081bbd",
   "metadata": {},
   "source": [
    "# Predicting Diabetes in Pima Indian Women Using Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8be4d5-b89d-4d82-8764-eff066fd63ce",
   "metadata": {},
   "source": [
    "By Inder Khera, Javier Martinez, Jenny Zhang & Jessica Kuo (alphabetically ordered), 2024/11/23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56ae3098-c2d3-49f8-8019-b8ed4ddd78df",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'altair'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01maltair\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01malt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01maltair_ally\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01maly\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline, make_pipeline\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'altair'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import altair_ally as aly\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import (\n",
    "    RandomizedSearchCV,\n",
    "    cross_validate,\n",
    "    cross_val_score,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f464a8d-35d0-4b26-8013-3f6481017712",
   "metadata": {},
   "source": [
    "# Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8377f0b6-b44c-4a63-94aa-d4f7ed1496dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Diabetes is a serious chronic disease characterized by high levels of glucose in the blood, caused by either insufficient insulin production by the pancreas or the bodyâ€™s inability to effectively use insulin. It has become a significant global health issue, with its prevalence nearly doubling since 1980, and in 2022, 14% of adults aged 18 and older were diagnosed with diabetes, doubling from 7% in 1990 (World Health Organization). Diabetes can lead to severe complications, including blindness, kidney failure, heart attacks, strokes, and lower limb amputations. Early detection enables timely interventions, reduces complications, lowers healthcare costs, and improves quality of life and long-term outcomes (Marshall & Flyvbjerg, 2006).\n",
    "\n",
    "Artificial intelligence (AI) leverages computer systems and big data to simulate intelligent behavior with minimal human intervention, and within it, machine learning (ML) is a subset of AI methodologies. Machine learning has increasingly been applied in various areas of disease detection and prevention in the healthcare field (Bini, 2018). Numerous machine learning techniques have been deployed to develop more efficient and effective methods for diagnosing chronic diseases (Battineni, Chinatalapudi, & Amenta, 2020). Utilizing machine learning methods in diabetes research is a critical strategy for harnessing large volumes of diabetes-related data to extract valuable insights (Agarwal & Vadiwala, 2022). Therefore, The goal of this report is to leverage a supervised machine learning model, logistic regression (LR), to evaluate its predictive performance in diagnosing diabetes using a real-world dataset focused specifically on Pima Indian women aged 21 and older."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5224d78-2cb5-42aa-868c-9c62b55ea98c",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e615259-0219-4e20-b64a-017acd79182e",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The dataset that was used for the analysis of this project was created by Jack W Smith, JE Everhart, WC Dickson, WC Knowler, RS Johannes and sourced from the National Librabry of Medicine database from the National Institues of Health. Access to their respective analysis can be found [here](https://pmc.ncbi.nlm.nih.gov/articles/PMC2245318/) and access to the dataset can be found via [kaggle](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database/data) (Dua & Graff, 2017). The primary objective of the dataset is to enable diagnostic prediction of whether a patient has diabetes based on specific diagnostic measurements. To ensure consistency and relevance, several constraints were applied to the selection of data instances. Specifically, the dataset includes only female patients who are at least 21 years old and of Pima Indian heritage.\n",
    "\n",
    "Each row/obersvation from the dataset is an individual that identifies to be a part of the Pima (also known as The Akimel O'odham) Indeginous group, located mainly in the Central and Southern regions of the United States. Each observation recorded has summary statistics regarding features that include the Age, BMI, Blood Pressure, Number of Pregnancies, as well as The Diabetes Pedigree Function (which is a score that gives an idea about how much correlation is between person with diabetes and their family history). The dataset offers comprehensive feastures for machine learning analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec82c1f-10ea-4e5e-8728-81d6c77b8053",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "Logistic Regression was employed to develop a classification model for predicting whether the patient is diabetic or not (as indicated in the `outcome` column of the dataset). All variables from the original dataset were used to train the model. The data was split into 70% for the training set and 30% for the test set. Hyperparameter tuning was performed using `RandomizedSearchCV`, with the accuracy score serving as the classification metric. All variables were standardized just before model fitting. The analysis was conducted using the Python programming language (Van Rossum and Drake, 2009) and several Python packages: numpy (Harris et al., 2020), Pandas (McKinney, 2010), altair (VanderPlas, 2018), altair_ally (Ostblom, 2021) and scikit-learn (Pedregosa et al., 2011). The code used for this analysis and report is available at: https://github.com/UBC-MDS/diabetes_predictor_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4c56b9-31ae-4b44-aba4-d4be1e4349b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/diabetes.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c19426-8ff5-4c00-aeb9-21335e7904fe",
   "metadata": {},
   "source": [
    "The `shape` attribute shows us the number of observations and the number of features in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97143fb-aad8-4ee4-9e0c-43e27912a360",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84b7f2f-44ac-4f95-9324-91a326914a16",
   "metadata": {},
   "source": [
    "The `info()` method shows that the data set does not have any features with missing values. It further shows that all features are numeric as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa0c09d-0057-4303-8ab1-d2848f7906f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b986adf-e01d-4e37-97f2-e0f5eb32060b",
   "metadata": {},
   "source": [
    "Using the `train_test_split()` function we will split our data set with 70% going to train the model and 30% going towards testing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0894088b-5714-4a20-bdb0-f4ffdf786dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df,\n",
    "                                     train_size = 0.7, \n",
    "                                     random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bcc2ba-a8de-4a68-9f29-308309a5025f",
   "metadata": {},
   "source": [
    "The `describe()` shows us the summary statistics of each of our features as well as our target value. We can see the mean as well as the spread (standard deviation). Using this information and the visualization tools we will see next we can determine how skewed each of our features are for their respective values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ceec33-f14b-4070-a225-186f75860095",
   "metadata": {},
   "outputs": [],
   "source": [
    "census_summary = train_df.describe()\n",
    "census_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35771738-ed67-4c0a-ade6-de3ff59304a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = census_summary.columns.tolist()\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dec8686-3519-49a2-8b6d-dfb254fb648e",
   "metadata": {},
   "source": [
    "feature_histograms = alt.Chart(train_df).transform_calculate(\n",
    ").mark_bar(opacity=0.5).encode( x = alt.X(alt.repeat()).type(\n",
    "'quantitative').bin(maxbins=30), y= alt.Y('count()').stack(False),\n",
    "                               color = 'Outcome:N'\n",
    ").properties( height=250,\n",
    "width=250 ).repeat(\n",
    "features, columns=2\n",
    ")\n",
    "\n",
    "feature_histograms "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6995241a-3405-46c1-a32c-51096604f88e",
   "metadata": {},
   "source": [
    "Figure 1. Comparison of the empirical distributions of training data predictors between those non-diabetic and diabetic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe80c9bb-631f-4ba8-b5ae-1256e15a3d92",
   "metadata": {},
   "source": [
    "Figure 1 above shows us the respective distribution of each of the features. We have categorized the results to show the how distribution of each feature is when the Outcome is 0 (Non-Diabetic) and when the Outcome is 1 (Diabetic). This helps give us an indication on certain relationships between the features and the target.\n",
    "\n",
    "For the Glucose levels, we see for the Non-Diabetic class that glucose levels are a somewhat normal distribution; but for the Diabetic class, the glucose levels lean heavily towards the middle to higher range. BMI for the Diabetic class looks like a normal distribution, but it also skews slighty to higher values. But for the Non-Diabetic class interestingly the BMI distribution seems more bimodal.\n",
    "\n",
    "For the distribution of Age we see that Ages 20 to 32 are dominated by Non-Diabetics, but after the age of 32 we see that the count levels are close between the Diabetic and Non-Diabetic classes, where for some bins the Diabetic class even overtakes the Non-Diabetic even with a lower total count of observations in the data set. The Non-Diabetic class in the Age Distribution leans more towards lower ages meanwhile the Diabetic class' distribution is somewhat consistent across its age range.\n",
    "\n",
    "For Pregnancies the lower range of pregnancies is dominated by the Non-Diabetic class, meanwhile for higher range of pregnancies the Diabetic class has more observations.\n",
    "\n",
    "For Skin Thickness both the Diabetic and Non-Diabetic class are close to a normal distribution but the Non-Diabetic distribution skews slighty towards lower values and the Diabetic class skews more towards higher values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9f6883-e835-4c89-9886-74402cecee80",
   "metadata": {},
   "source": [
    "aly.corr(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb988de9-549c-4594-ace9-9fa6436bbd27",
   "metadata": {},
   "source": [
    "Figure 2: Pearson and Spearman correlations across all features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43927d3-04dc-433f-8c90-d7c662826730",
   "metadata": {},
   "source": [
    "Figure 2 shows the correlation between all of the respective features. The main reasoning to analyze thi is to see if there is any multicollinearity between any of the features which is problamatic when conducting a Logistic Regression. We see that highest level of correlation is between Age and Pregnancies (0.53 by Pearson, and 0.59 via Spearman). Since this is below the threshold of 0.7 we can conclude that all feature coefficients are suitable and will not cause any multicollinearity in our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd50b563-9985-4d2c-8a3f-e9a97f2a6fb1",
   "metadata": {},
   "source": [
    "aly.pair(train_df[features].sample(300), color='Outcome:N')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c2d626-c338-4ff2-8de3-8dee97a2e1a7",
   "metadata": {},
   "source": [
    "Figure 3: Pairwise scatterplots between each of features in dataset to visualize relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567fa9f0-a7b0-4408-8b17-39259c51688c",
   "metadata": {},
   "source": [
    "Figure 3 above gives us a visualization between the relationship between each of our features. We see for the most part that the features do not show and trends. The two features that do show somewhat of a relationship visually is Skin thickness and BMI. This would makes sense as the higher the body mass the higher the thickness of skin would be for the most part. \n",
    "\n",
    "Looking back at our previous at the correlation graph from before we see that Skin Thickness and BMI have a Pearson correlation of 0.41, meaning they do not cause multicollinearity in our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e9d502-d7de-4947-9aef-7237844abf57",
   "metadata": {},
   "source": [
    "Here we further split our data set into our X and y for both the training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3c1217-fed7-4e33-a18f-4d510e6af191",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(columns = ['Outcome'])\n",
    "y_train = train_df['Outcome']\n",
    "X_test = test_df.drop(columns = ['Outcome'])\n",
    "y_test = test_df['Outcome']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14944f93-65c4-439c-b61d-a515fc5e2e31",
   "metadata": {},
   "source": [
    "We have created a Dummy Classifier to act as our base line for conductin our analysis.\n",
    "The Dummy Baseline gives us a score of around 0.6648"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11f6174-f5d6-4e45-b857-2bfb65a80e9a",
   "metadata": {},
   "source": [
    "dummy_clf = DummyClassifier()\n",
    "mean_cv_score = cross_val_score(dummy_clf, \n",
    "                                X_train,\n",
    "                                y_train).mean()\n",
    "mean_cv_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4249d6d-92b8-4ab5-9843-b0bfeebf5475",
   "metadata": {},
   "source": [
    "We will be using a Logistic Regression model to do our classification. Since our features have outliers it would be best to use a StandardScaler() to normalize the feature values before fitting the model to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f54d5b3-b4b3-4a69-9080-976088f33aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pipe=make_pipeline(StandardScaler(), LogisticRegression(max_iter=2000,\n",
    "random_state=123))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0644fd-9d0d-4e6b-b732-11d735a769d8",
   "metadata": {},
   "source": [
    "We optimze the hyperparamter 'C' for our logistic regression using a random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10857b9-e68b-4427-90d0-65638136b431",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "param_dist = {\n",
    "    \"logisticregression__C\": [10**i for i in range(-5,15)] \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51de2544-ffac-4b1b-970a-e1920e03b508",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search = RandomizedSearchCV(log_pipe,param_dist,\n",
    "                                   n_iter=20,\n",
    "                                   n_jobs=-1,\n",
    "                                   return_train_score=True,\n",
    "                                   random_state=123)\n",
    "\n",
    "random_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc4ce0d-dbf8-4473-8a0d-a5b9229e363d",
   "metadata": {},
   "source": [
    "We find out best parameter value for our hyperparameter `C` that we will use in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64337e46-03eb-4182-bde3-4b7efb3f33e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = random_search.best_params_ \n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b463a207-970a-4b87-b183-165a270216cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(random_search.cv_results_).sort_values(\n",
    "    \"rank_test_score\").head(3)[[\"mean_test_score\",\n",
    "                                \"mean_train_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897f9bdd-d070-4595-8bba-525bcce98875",
   "metadata": {},
   "source": [
    "Having determined the best Logistic Regression model for our analysis, we further explore feature importance with coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8c81cf-262e-4ac9-95eb-7b89a0e15385",
   "metadata": {},
   "source": [
    "# Best model from the search\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Retrieve the coefficients and feature names\n",
    "coefficients = best_model.named_steps['logisticregression'].coef_.flatten()\n",
    "features = X_train.columns  \n",
    "\n",
    "# Create a DataFrame to display the feature names and corresponding coefficients\n",
    "coeff_df = pd.DataFrame({\n",
    "    'Features': features,\n",
    "    'Coefficients': coefficients\n",
    "})\n",
    "\n",
    "# Sort by 'Coefficients' in descending order to see the most important features first\n",
    "coeff_df_sorted = coeff_df.sort_values(by = 'Coefficients', ascending = False)\n",
    "\n",
    "# Create a heatmap for the coefficients (we will visualize them as a single column)\n",
    "coeff_df_sorted.style.format(\n",
    "    precision = 2\n",
    ").background_gradient(\n",
    "    axis = None,\n",
    "    cmap = 'RdBu_r',\n",
    "    low = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da25f622-c18d-4042-88c5-c68544ce2fae",
   "metadata": {},
   "source": [
    "Table 1: Logistic regression feature importance measured by coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70be6981-1268-4215-81f1-acfee385e7d6",
   "metadata": {},
   "source": [
    "Based on the heatmap and table 1 above, the feature importance coefficients for the logistic regression model predicting diabetes reveal that `Glucose` (1.08) is the strongest positive influence, followed by `BMI` (0.72) and `Pregnancies` (0.39). The negative influences `BloodPressure` (-0.21) and `Insulin` (-0.20) along with the remaining postive features `DiabetesPedigreeFunction` (0.29), `Age` (0.12), and `SkinThickness` (0.04), have a moderate to weak impact on the prediction, with their effects being less pronounced. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60168556-9d5e-494a-a77f-b83ca2c2f991",
   "metadata": {},
   "source": [
    "We then perform best Logistic Regression model from hyperparameter search on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569ec32e-1d4f-413a-84ba-38359e62a0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the best model\n",
    "y_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97066ce3-28e9-4462-984b-6d045389fa50",
   "metadata": {},
   "source": [
    "In addition, to enhance the model's practical use in a clinical setting, we are providing and reporting probability estimates for the predictions of diabetes. Offering probability estimates would allow clinicians to gauge the model's confidence in its predictions. This would give clinicians the opportunity to conduct additional diagnostic tests if the predicted probability for the outcome (i.e. diagnosis of prediction) is not sufficiently high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f712d3-c220-4f1d-9f4e-75eced342c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = best_model.predict_proba(X_test)\n",
    "pred_bool = (y_test == y_pred)\n",
    "pred_results_1 = np.vstack([y_test, y_pred, pred_bool, y_pred_prob[:, 1]])\n",
    "pred_results_1_df = pd.DataFrame(pred_results_1.T, \n",
    "                                 columns = ['y_test', 'y_pred', 'pred_bool', 'y_pred_prob_1'])\n",
    "pred_results_1_df['pred_bool'] = pred_results_1_df['pred_bool'] == 1\n",
    "pred_results_1_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae4aa67-7176-4eed-9170-2962f09aa935",
   "metadata": {},
   "source": [
    "Our prediction model performed decent on test data, with a final overall accuracy of 0.80. In addition, looking through the prediction results dataframe, there are a total of 46 mistakes. Of which, 31 mistakes were predicting diabetic as non-diabetic (false negatives) and 15 mistakes were made predicting diabetic as non-diabetic (false positives). Considering implementation in clinic, there is room for improvement in the algorithm as false negatives are more harmful than false positives, and we should aim to lower false positives even further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30250d8d-40d4-41e3-8560-48d2b8a56673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy\n",
    "accuracy = best_model.score(X_test, y_test)\n",
    "\n",
    "pd.DataFrame({'accuracy': [accuracy]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e43428-628a-4495-b078-60568d4276e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of correct predictions and misclassifications\n",
    "value_counts = pred_results_1_df['pred_bool'].value_counts()\n",
    "\n",
    "pd.DataFrame({\n",
    "    'correct predictions': [value_counts.get(True, 0)], \n",
    "    'misclassifications': [value_counts.get(False, 0)]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056f09d2-a81b-42a4-bcaa-3837aeee29b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of false positives (FPs) and false negatives (FNs)\n",
    "fp = len(pred_results_1_df[(pred_results_1_df['y_test'] == 0) & (pred_results_1_df['y_pred'] == 1)])\n",
    "fn = len(pred_results_1_df[(pred_results_1_df['y_test'] == 1) & (pred_results_1_df['y_pred'] == 0)])\n",
    "\n",
    "pd.DataFrame({\n",
    "    'false positives': [fp], \n",
    "    'false negatives': [fn]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f77a691-b4fc-41fb-850a-670064fcbc83",
   "metadata": {},
   "source": [
    "Moreover, visualizing prediction probabilities alongside the prediction accuracy for each test sample provides a clearer understanding of the model's performance. This approach allows us to easily assess how well the model predicts, while also highlighting patients who were misdiagnosed. Particularly, it helps us focus on false negatives, as the consequences of these errors are more critical in a clinical context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35218b3b-5536-40f5-909b-5e9f1374d49b",
   "metadata": {},
   "source": [
    "alt.Chart(pred_results_1_df, title = 'Test Set Prediction Accuracy').mark_tick().encode(\n",
    "    x = alt.X('y_pred_prob_1').title('Positive Class Prediction Prob'),\n",
    "    y = alt.Y('pred_bool').title('Pred. Accuracy'),\n",
    "    color = alt.Color('y_test:N').title('Outcome')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355be816-f4fe-4416-920c-7f8d922daac3",
   "metadata": {},
   "source": [
    "Figure 4: Test Set Prediction Accuracy by Prediction Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5157e9-de23-4f0f-bdcd-7e3e508f3572",
   "metadata": {},
   "source": [
    "While the performance of this model may be valuable as a screening tool in a clinical context, especially given its improvements over the baseline, there are several opportunities for further enhancement.\n",
    "\n",
    "One potential approach is to closely examine the 46 misclassified observations, comparing them with correctly classified examples from both classes. The objective would be to identify which features may be contributing to the misclassifications and investigate whether feature engineering could help the model improve its predictions on the observations it is currently struggling with.\n",
    "\n",
    "Additionally, we would try seeing whether we can get improved predictions using other classifiers. Other classifiers we might try are 1) random forest because it automatically allows for feature interaction, 2) k-nearest neighbours (k-NN) which usually provides easily interpretable and decent predictions, and 3) support vector classifier (SVC) as it allows for non-linear prediction using the rbf kernel. \n",
    "\n",
    "Moreover, there runs the possibility that the features offered from this dataset alone are not sufficient to predict with high accuracy. In this case, conducting additional conversations with data collectors for additional useable information or explore additional datasets that can be joined so our set of features can be expanded for more complicated analysis might be beneficial. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973fcb4d-ee7e-4367-a0d6-8e674bf4eb85",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a22e58-2428-4858-bef1-c90c600813d1",
   "metadata": {},
   "source": [
    "Agarwal, N., & Vadiwala, R. (2022). Machine Learning and Data Mining Methods in Diabetes Research. Asian Journal of Organic & Medicinal Chemistry.\n",
    " \n",
    "Battineni, G., Sagaro, G. G., Chinatalapudi, N., & Amenta, F. (2020). Applications of machine learning predictive models in the chronic disease diagnosis. Journal of personalized medicine, 10(2), 21.\n",
    " \n",
    "Bini, S. A. (2018). Artificial intelligence, machine learning, deep learning, and cognitive computing: what do these terms mean and how will they impact health care?. The Journal of arthroplasty, 33(8), 2358-2361.\n",
    " \n",
    "Dua, D., & Graff, C. (2017). Pima Indians Diabetes Database. UCI Machine Learning Repository. Retrieved from https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database/data\n",
    " \n",
    "Marshall, S. M., & Flyvbjerg, A. (2006). Prevention and early detection of vascular complications of diabetes. Bmj, 333(7566), 475-480.\n",
    "  \n",
    "World Health Organization. (n.d.). Diabetes. Retrieved November 22, 2024, from https://www.who.int/news-room/fact-sheets/detail/diabetes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
