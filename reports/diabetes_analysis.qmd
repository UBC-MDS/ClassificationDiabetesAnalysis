---
title: Predicting Diabetes in Pima Indian Women Using Logistic Regression
author: "Inder Khera, Jenny Zhang, Jessica Kuo, Javier Martinez"
date: "2024/12/15"
jupyter: python3
format: 
    html:
        toc: true
        toc-depth: 3
    pdf:
        toc: true
        toc-depth: 3
        fig-pos: "H"
format-links:
    - html
    - text: PDF
      href: diabetes_analysis.pdf
bibliography: references.bib
execute:
  echo: false
  warning: false
editor: source
---

```{python}
import pandas as pd
import numpy as np
import pickle
from IPython.display import Markdown, display
from tabulate import tabulate

diabetes = pd.read_csv("../data/raw/diabetes.csv")
diabetes_validated = pd.read_csv("../data/processed/diabetes_validated.csv")
dropped_obs = diabetes.shape[0] - diabetes_validated.shape[0]

pearson_corr = diabetes_validated.corr(method = "pearson")
spearman_corr = diabetes_validated.corr(method = "spearman")

max_pearson_corr = np.max(np.triu(pearson_corr, k=1))
max_spearman_corr = np.max(np.triu(spearman_corr, k=1))
corr_skin_bmi = spearman_corr.loc["SkinThickness", "BMI"]


mean_cv_score = pd.read_csv("../results/tables/mean_cv_score.csv")
best_param = pd.read_csv("../results/tables/best_params.csv")
mean_scores = pd.read_csv("../results/tables/mean_scores.csv")
coeff_table = pd.read_csv("../results/tables/coeff_table.csv")

test_scores_df = pd.read_csv("../results/tables/test_scores_df.csv")
confusion_matrix_df = pd.read_csv("../results/tables/confusion_matrix_df.csv")
value_counts_df = pd.read_csv("../results/tables/value_counts_df.csv")
pred_results_1_df =  pd.read_csv("../results/tables/pred_results_1_df.csv")

with open('../results/models/log_pipe.pkl', 'rb') as f:
    log_pipe = pickle.load(f)
with open('../results/models/random_fit.pkl', 'rb') as s:
    random_fit = pickle.load(s)

```

# Summary

In this study, we aim to address the question of whether clinical features such as glucose levels, BMI, pregnancies, and etc.
can effectively predict diabetes in Pima Indian women. The importance of this work lies in improving the early detection of diabetes, 
a condition that is prevalent and can lead to severe health complications if left untreated. 
Early prediction and diagnosis of diabetes can significantly improve patient outcomes through timely intervention.

We built a logistic regression model with hyperparameter optimization for C, and evaluated its performance on the test set. 
The final classifier achieved an accuracy of `{python} f"{test_scores_df['accuracy'].values[0]:.3f}"` on the test set, 
outperforming the baseline dummy classifier's accuracy of `{python} f"{mean_cv_score['mean_cv_score'].values[0]:.3f}"`. 
Glucose was the most significant predictor, followed by BMI and pregnancies, while blood pressure and insulin had weaker impacts. 
Out of a total of `{python} confusion_matrix_df.sum(axis=1).sum()` test cases, the model correctly predicted 
`{python} confusion_matrix_df['0'].values[0] + confusion_matrix_df['1'].values[1]` 
and misclassified `{python} confusion_matrix_df['0'].values[1] + confusion_matrix_df['1'].values[0]`.
`{python} confusion_matrix_df['0'].values[1]` mistakes were predicting patients with diabetes as non-diabetic (i.e. false negatives), while 
`{python} confusion_matrix_df['1'].values[0]` mistakes were predicting healthy (non-diabetic) patients with diabetes (i.e. false positives).

The results indicate that logistic regression is a promising tool for diabetes screening, providing an efficient way to identify potential cases. 
However, the high number of false negatives is concerning, as they could lead to delayed diagnoses and treatments. 
The use of logistic regression itself has some limitations as it assumes linear relationships and may not capture complex interactions between features. 
Future improvements could include feature engineering of polynomial features, testing alternative machine learning models, 
reporting more metrics to reflect model performance (i.e. recall and / or f2 score to focus on reducing false negatives), 
and incorporating additional data, such as lifestyle or genetic factors. 
Moreover, adding probability estimates for predictions could also enhance its clinical usability by helping prioritize further diagnostic tests. 
These steps could make the model more reliable and practical for real-world healthcare applications.

# Introduction

Diabetes is a serious chronic disease characterized by high levels of glucose in the blood, 
which can result from insufficient insulin production or the bodyâ€™s inability to effectively use insulin. 
Its prevalence has nearly doubled since 1980, with 14% of adults aged 18 and older diagnosed with diabetes in 2022, up from 7% in 1990 [@who_diabetes]. 
The disease can lead to severe complications, including blindness, kidney failure, heart attacks, strokes, and lower limb amputations. 
Early detection allows for timely interventions, reducing complications and healthcare costs, and improving quality of life and long-term outcomes [@marshall2006prevention].

Artificial intelligence (AI) leverages computer systems and big data to simulate intelligent behavior with minimal human intervention, 
and within it, machine learning (ML) is a subset of AI methodologies. Since the rise of AI, 
Machine learning has increasingly been applied in various areas of disease detection and prevention in the healthcare field [@bini2018artificial]. 
Numerous machine learning techniques have been deployed to develop more efficient 
and effective methods for diagnosing chronic diseases [@battineni2020applications]. 
Utilizing machine learning methods in diabetes research has been proven to be a critical strategy 
or harnessing large volumes of diabetes-related data to extract valuable insights [@agarwal2022machine]. 

This study aims to address the research question: can diabetes in Pima Indian women be accurately predicted using clinical features 
such as glucose levels, BMI, and pregnancies? The [dataset](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database/data) used for this analysis, 
the Pima Indians Diabetes Database, contains clinical data for 768 women aged 21 and older, with 8 input variables: number of pregnancies, 
plasma glucose concentration, iastolic blood pressure, triceps skinfold thickness, serum insulin, body mass index (BMI), diabetes pedigree function, and age. 
The output variable is whether or not the woman has diabetes, with two classes: positive (1) or negative (0). 
To this end, the objective of this report is to evaluate the predictive performance of logistic regression (LR), 
a supervised machine learning model, in diagnosing diabetes. 


# Methods

## Data


The dataset that was used for the analysis of this project was created by Jack W Smith, JE Everhart, WC Dickson, WC Knowler, RS Johannes 
and sourced from the National Librabry of Medicine database from the National Institues of Health. 
Access to their respective analysis can be found [here](https://pmc.ncbi.nlm.nih.gov/articles/PMC2245318/) 
and access to the dataset can be found via [kaggle](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database/data) [@dua2017pima]. 
The primary objective of the dataset is to enable diagnostic prediction of whether a patient has diabetes based on specific diagnostic measurements. 
The dataset comprises 768 female patients aged 21 and older, all of whom are of Pima Indian heritage, 
located primarily in the Central and Southern regions of the United States. 

Each row/obersvation from the dataset is an individual that identifies to be a part of the Pima (also known as The Akimel O'odham) Indeginous group.
Each observation recorded has summary statistics regarding features that include the Age, BMI, Blood Pressure, Number of Pregnancies, 
as well as The Diabetes Pedigree Function (which is a score that gives an idea about how much correlation is between person with diabetes and their family history). 
The output variable in the dataset is whether the patient is diabetic (outcome), with two classes: diabetic (1) or non-diabetic (0). 
The dataset is imbalanced, with 268 diabetic and 500 non-diabetic patients. This imbalance poses challenges for classification models, 
which may become biased toward predicting the majority class. Usually to address this, appropriate evaluation metrics, such as F1 score, 
should be considered during model evaluation and hyperparameter tuning to avoid the model being skewed towards the non-diabetic class.

A total of `{python} dropped_obs` observations were dropped during preliminary data validation. 
These rows contained meaningless or introducing noise or spurious relationships into the model. 
Further details on the dropped observations can be found in the validation log [here](https://github.com/UBC-MDS/diabetes_predictor_py/tree/main/reports), under `validation_errors.log`.
The log indicates that most of the dropped data points contain values of 0, which are not plausible for the respective variables. 
As a result, we suspect these zero values may represent missing data, recorded in this way. 
If we gain access to the data collectors, we plan to confirm this assumption.


## Analysis

In this study, Logistic Regression was employed to develop a classification model for predicting whether a patient has diabetes. 
The model was trained using all features in the dataset, with the outcome column serving as the target variable. 
The data was split into a 70% training set and a 30% testing set. 

Hyperparameter tuning was performed using `RandomizedSearchCV`, and the accuracy score was used as the evaluation metric for model performance. 
The hyperparameter C of the Logistic Regression model, which controls the trade-off between model complexity and training data fitting, 
was optimized using a log-uniform distribution ranging from \(1 \times 10^{-5}\) and \(1 \times 10^{5}\).
Standardization was applied to all input variables just before model fitting to ensure that the features were on the same scale. 
This process was done to improve model performance and to help prevent any single feature from dominating the others.
The analysis was conducted using the Python programming language [@Python] and several Python packages: numpy [@harris2020array], 
Pandas [@pandas], altair [@altair], altair_ally [@ostblom2021altair] and scikit-learn [@pedregosa2011scikit]. 
The code used for this analysis and report is available at: https://github.com/UBC-MDS/diabetes_predictor_py


# Results

To assess the potential usefulness of each predictor in forecasting the Outcome variable: 0 (Non-Diabetic) and 1 (Diabetic), we visualized the distributions of each predictor from the training dataset, 
with the distributions color-coded by class (0: blue, 1: orange) as shown in @fig-feature_histograms.

For the `Glucose` levels, Non-Diabetic class exhibits a roughly normal distribution, 
whereas the Diabetic class shows a pronounced shift toward the middle-to-higher range of glucose levels. 

The `BMI` distribution resembles a normal distribution but skews slightly toward higher values. 
Similar to Glucose levels, the Diabetic class displays a decent shit towards the middle-to-higher ranges when compared to Non-Diabetic class, 
suggesting the potential for distinct differences between target groups within this category.

The `Age` distribution reveals that individuals aged 20 to 32 are predominantly Non-Diabetic. 
Beyond age 32, the counts of Diabetic and Non-Diabetic individuals become comparable, 
with some bins showing a higher count for the Diabetic class, despite fewer overall observations in this group. 
The Non-Diabetic class leans toward younger ages, while the Diabetic class has a more even distribution across its age range.

For `Pregnancies`, `Insulin`, and `DiabetesPedigreeFunction` - genetic risk of diabetes based on family history ranging from 0 to 2.5, 
the lower range of pregnancies is dominated by the Non-Diabetic class, whereas whereas higher numbers are more common in the Diabetic class.

For `Blood Pressure` and `Skin Thickness`, both the Diabetic and Non-Diabetic classes approximates a normal distribution; 
however, the Non-Diabetic distribution skews slightly towards lower values, while the Diabetic class skews more towards higher values.


![Comparison of the empirical distributions of training data predictors between those non-diabetic and diabetic.](../results/figures/feature_histograms.png){#fig-feature_histograms width=80%}


We also examined the presence of multicollinearity among the predictors in @fig-correlation_heatmap, as it could be problematic when conducting a Logistic Regression. 
We see that highest level of correlation is between Age and Pregnancies (`{python} f"{max_spearman_corr:.3f}"` via Spearman, and `{python} f"{max_pearson_corr:.3f}"` by Pearson). 
Since this is below the threshold of 0.7, we can conclude that all features' coefficients are suitable and will not cause any multicollinearity in our model. 


![Pearson and Spearman correlations across all features.](../results/figures/correlation_heatmap.png){#fig-correlation_heatmap width=80%}


Finally, we looked at the pairwise scatterplots between features in @fig-pairwise_scatterplot to detect any additional patterns. 
For the most part, the features do not display noticeable trends. However, Skin Thickness and BMI show a moderate visual relationship, 
which is intuitive since higher body mass is generally associated with increased skin thickness. 


![Pairwise scatterplots between each of features in dataset to visualize relationship.](../results/figures/pairwise_scatterplot.png){#fig-pairwise_scatterplot width=110%}


After exploratory data analysis and validation, We then further split our dataset into X and y for both the training and test. 
We used the Dummy Classifier to act as our baseline for conducting our initial analysis. 
The Dummy Baseline gives us a score of around `{python} f"{mean_cv_score['mean_cv_score'].values[0]:.3f}"`.

We then used Logistic Regression model for classification. Given the presence of outliers in our features, 
it is advisable to apply `StandardScaler()` to normalize the feature values before fitting the model. 
This ensures that all features are on a similar scale, improving the model's performance and stability.
We optimized the hyperparameter `C` using a random search approach and have identified C = `{python} f"{best_param['logisticregression__C'].values[0]:.3f}"` 
as the optimal C to be used in our Logistic Regression model.


```{python}
#| label: tbl-coeff_table
#| tbl-cap: Logistic regression feature importance measured by coefficients.
#| echo: false
Markdown(coeff_table.to_markdown())
```

Having determined the best Logistic Regression model for our analysis, we further explore feature importance with coefficients. 
Based on the heatmap and @tbl-coeff_table above, the feature importance coefficients for the logistic regression model predicting diabetes reveal 
that `Glucose` (`{python} round(coeff_table.iloc[coeff_table[coeff_table.iloc[:, 0] == 'Glucose'].index[0], 1], 3)`) is the strongest positive influence, 
followed by `BMI` (`{python} round(coeff_table.iloc[coeff_table[coeff_table.iloc[:, 0] == 'BMI'].index[0], 1], 3)`), 
`Pregnancies` (`{python} round(coeff_table.iloc[coeff_table[coeff_table.iloc[:, 0] == 'Pregnancies'].index[0], 1], 3)`), 
`Age` (`{python} round(coeff_table.iloc[coeff_table[coeff_table.iloc[:, 0] == 'Age'].index[0], 1], 3)`), 
and `DiabetesPedigreeFunction` (`{python} round(coeff_table.iloc[coeff_table[coeff_table.iloc[:, 0] == 'DiabetesPedigreeFunction'].index[0], 1], 3)`). 
The negative influence `SkinThickness` (`{python} round(coeff_table.iloc[coeff_table[coeff_table.iloc[:, 0] == 'SkinThickness'].index[0], 1], 3)`) 
along with the remaining positive features `BloodPressure` (`{python} round(coeff_table.iloc[coeff_table[coeff_table.iloc[:, 0] == 'BloodPressure'].index[0], 1], 3)`) 
and `Insulin` (`{python} round(coeff_table.iloc[coeff_table[coeff_table.iloc[:, 0] == 'Insulin'].index[0], 1], 3)`), 
have weak impacts on the prediction, with their effects being less pronounced. 

We then evaluate the best Logistic Regression model, obtained from the hyperparameter search, on the test set. 
Our prediction model performed decent on test data, with a final overall accuracy of `{python} f"{test_scores_df['accuracy'].values[0]:.3f}"`. 
In addition, looking through confusion matrix (@fig-test_confusion_matrix), there are a total of `{python} confusion_matrix_df['0'].values[1] + confusion_matrix_df['1'].values[0]` mistakes. 
Of which, `{python} confusion_matrix_df['0'].values[1]` mistakes were predicting diabetic as non-diabetic (false negatives) 
and `{python} confusion_matrix_df['1'].values[0]` mistakes were made predicting diabetic as non-diabetic (false positives). 
Considering implementation in clinic, there is room for improvement in the algorithm as false negatives are more harmful than false positives, 
and we should aim to lower false positives even further.


![Confusion Matrix of Test Set Prediction Accuracy](../results/figures/confusion_matrix_plot.png){#fig-test_confusion_matrix width=60%}


In this report, we have used the default 0.5 threshold in Logistic Regression model to predict the patient being diabetic or non-diabetic. 
To better evaluate model's performance across all thresholds, we also presented here the Precision Recall curve (@fig-test_pr) 
and the ROC curve (@fig-test_roc) - assessing the tradeoff between true positive and false positive rates. 
For both plots, we did not observe an optimal threshold that can achieve high precision, high recall, and low false positive rate all at once. 
Therefore, further improvements on the Logistic Regression model or alternative models should be contemplated in further research.


<div style="display: flex; justify-content: space-between;">
  <figure style="width: 80%;">
    ![Precision Recall Curve of Test Set Predictions](../results/figures/precision_recall_plot.png){#fig-test_pr width=60%}
  </figure>
  <figure style="width: 80%;">
    ![ROC Curve of Test Set Predictions](../results/figures/roc_curve.png){#fig-test_roc width=60%}
  </figure>
</div>


To improve the model's clinical utility, we provide a visualization of estimated prediction probabilities (@fig-test_pred_accur) for diabetes predictions. 
This allows clinicians to assess the model's confidence in its predictions and decide whether additional diagnostic tests are needed 
if the probability is not sufficiently high. Visualizing these probabilities alongside prediction accuracy offers a clearer understanding of the model's performance, 
highlighting both correct predictions and misdiagnoses, especially false negatives, which are of particular concern in a clinical setting due to their critical consequences.


![Test Set Prediction Accuracy by Prediction Probability.](../results/figures/predict_chart.png){#fig-test_pred_accur width=60%}


# Discussion

While the performance of this model may be valuable as a screening tool in a clinical context, 
especially given its improvements over the baseline, there are several opportunities for further enhancement. 
One potential approach is to closely examine the `{python} confusion_matrix_df['0'].values[1] + confusion_matrix_df['1'].values[0]` misclassified observations, 
comparing them with correctly classified examples from both classes. The objective would be to identify 
which features may be contributing to the misclassifications and investigate 
whether feature engineering could help the model improve its predictions on the observations it is currently struggling with. 
Additionally, we would try seeing whether we can get improved predictions using other classifiers. 
Other classifiers we might try are 1) random forest because it automatically allows for feature interaction, 
2) k-nearest neighbours (k-NN) which usually provides easily interpretable and decent predictions, 
and 3) support vector classifier (SVC) as it allows for non-linear prediction using the rbf kernel. 
Finally, there runs the possibility that the features offered from this dataset alone are not sufficient to predict with high accuracy. 
In this case, conducting additional conversations with data collectors for additional useable information or explore additional datasets 
that can be joined so our set of features can be expanded for more complicated analysis might be beneficial. 

At last, we recognize the limitation with this dataset, as it focuses solely on Pima Indian women aged 21 and older, 
which limits its generalizability to other populations. To improve the analysis, it would be valuable to combine this data 
with other datasets representing different age groups, genders, and ethnicities, enabling more comprehensive insights and broader applicability of the findings.

# Conclusion

In conclusion, this study demonstrated the effectiveness of logistic regression in predicting diabetes among Pima Indian women 
using diagnostic features such as glucose, BMI, and pregnancies. With an accuracy of `{python} f"{test_scores_df['accuracy'].values[0]:.3f}"` on the test set, 
the model outperformed the baseline Dummy Classifier's `{python} f"{mean_cv_score['mean_cv_score'].values[0]:.3f}"`. 
Glucose was identified as the most influential predictor, followed by BMI and pregnancies, 
while features like blood pressure, insulin, and skin thickness had weaker impacts. 
However, the model's `{python} confusion_matrix_df['0'].values[1] + confusion_matrix_df['1'].values[0]` misclassifications, 
particularly the `{python} confusion_matrix_df['0'].values[1]` false negatives, 
underscore the need for further refinement to minimize the risk of undiagnosed cases.

These findings highlight logistic regression's potential as an initial screening tool in clinical settings, offering a data-driven approach to early diabetes detection. 
Nevertheless, improvements are essential to enhance its accuracy and practical utility. 
Strategies such as feature engineering, alternative machine learning models, and the incorporation of additional data, 
such as lifestyle or genetic factors, could further optimize performance. 
Additionally, providing probability estimates for predictions could enhance clinical decision-making by identifying cases requiring further diagnostics. 
With these refinements, the model could become a valuable tool for reducing complications and improving outcomes in diabetes care.

# References

